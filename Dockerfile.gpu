FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04

LABEL maintainer="Open WebUI (Timothy Jaeryang Baek)"
LABEL description="LlamaCpp Runner - GPU accelerated version"
LABEL version="0.0.1"

WORKDIR /app

# Install essential packages and build dependencies with proper cleanup in a single layer
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    wget \
    git \
    build-essential \
    cmake \
    python3.11 \
    python3.11-dev \
    python3-pip \
    python3-setuptools \
    python3-wheel \
    patchelf \
    findutils \
    ca-certificates \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.11 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 && \
    update-alternatives --set python3 /usr/bin/python3.11 && \
    ln -sf /usr/bin/python3 /usr/bin/python

# Ensure pip is up to date
RUN python3 -m pip install --no-cache-dir --upgrade pip setuptools wheel

# Copy only necessary files for installation
COPY pyproject.toml README.md LICENSE /app/
COPY src/ /app/src/

# Install basic dependencies first
RUN pip3 install --no-cache-dir "requests>=2.28.0" "fastapi>=0.95.0" "uvicorn>=0.21.0"

# Then install the package in development mode
RUN pip3 install --no-cache-dir -e .

# Install PyTorch (optional, for improved compatibility with some models)
RUN pip3 install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Create volume mount points
VOLUME /models
VOLUME /cache

# Copy setup script
COPY scripts/setup_libs.sh /app/setup_libs.sh
RUN chmod +x /app/setup_libs.sh

# Create a startup script that sets up the environment
RUN echo '#!/bin/bash\n\
echo "Setting up GPU environment..."\n\
\n\
# Run library setup script\n\
/app/setup_libs.sh\n\
\n\
# Add library paths to LD_LIBRARY_PATH\n\
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/cache/llama_cpp/build/lib:/cache/llama_cpp/build/bin:/usr/local/cuda/lib64\n\
\n\
# Check GPU availability\n\
if [ "$ENABLE_GPU" = "true" ]; then\n\
    if command -v nvidia-smi &> /dev/null; then\n\
        echo "NVIDIA GPU detected:"\n\
        nvidia-smi\n\
    else\n\
        echo "WARNING: NVIDIA drivers not found but GPU mode is enabled!"\n\
        echo "Will attempt to continue, but GPU acceleration may not work."\n\
    fi\n\
else\n\
    echo "GPU support disabled by configuration."\n\
fi\n\
\n\
# Print current environment\n\
echo "Environment:"\n\
echo "  MODELS_DIR=$MODELS_DIR"\n\
echo "  CACHE_DIR=$CACHE_DIR"\n\
echo "  TIMEOUT_MINUTES=$TIMEOUT_MINUTES"\n\
echo "  ENABLE_GPU=$ENABLE_GPU"\n\
echo "  GPU_LAYERS=$GPU_LAYERS"\n\
echo "  PORT=$PORT"\n\
echo "  HOST=$HOST"\n\
echo "  LOG_LEVEL=$LOG_LEVEL"\n\
\n\
# Start the server\n\
echo "Starting server..."\n\
if [ "$ENABLE_GPU" = "true" ]; then\n\
    python -m llama_cpp_runner.main --models-dir "$MODELS_DIR" --cache-dir "$CACHE_DIR" \\\n\
        --port "$PORT" --host "$HOST" --timeout "$TIMEOUT_MINUTES" \\\n\
        --log-level "$LOG_LEVEL" --verbose --gpu --gpu-layers "$GPU_LAYERS"\n\
else\n\
    python -m llama_cpp_runner.main --models-dir "$MODELS_DIR" --cache-dir "$CACHE_DIR" \\\n\
        --port "$PORT" --host "$HOST" --timeout "$TIMEOUT_MINUTES" \\\n\
        --log-level "$LOG_LEVEL" --verbose\n\
fi\n' > /app/start.sh && chmod +x /app/start.sh

# Expose the API port
EXPOSE 10000

# Set environment variables with reasonable defaults
ENV PYTHONUNBUFFERED=1
ENV MODELS_DIR=/models
ENV CACHE_DIR=/cache
ENV VERBOSE=true
ENV TIMEOUT_MINUTES=30
ENV ENABLE_GPU=true
ENV GPU_LAYERS=-1
ENV PORT=10000
ENV HOST=0.0.0.0
ENV LOG_LEVEL=info
ENV CUDA_VISIBLE_DEVICES=0

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:${PORT}/health || exit 1

# Command to run when the container starts
CMD ["/app/start.sh"]